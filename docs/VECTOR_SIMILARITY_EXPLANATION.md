# Embedding 기반 벡터 유사도 측정

## 📊 개요

**목적**: 사용자 입력 메시지와 가장 유사한 Tone Sample을 찾아 답변 생성에 활용

**방법**:
1. 입력 메시지를 1536차원 벡터로 변환 (OpenAI Embedding)
2. 저장된 Tone Sample 벡터들과 유사도 계산
3. 유사도 상위 10개 Tone Sample 선택
4. 선택된 샘플을 LLM에 제공하여 답변 생성

---

## 📐 벡터 유사도 측정 방법

### 1. 코사인 유사도 (Cosine Similarity)

**공식**:
```
similarity = (A · B) / (||A|| × ||B||)
```

**설명**:
- 두 벡터 간의 **각도**를 측정
- 방향이 같으면 1, 반대면 -1, 직각이면 0
- 의미적으로 유사한 문장일수록 1에 가까움

**장점**:
- 문장 길이에 영향받지 않음
- 의미적 유사성을 잘 포착

**예시**:
```
쿼리: "안녕?"
샘플1: "안녕하세요" → 유사도 0.85
샘플2: "잘가" → 유사도 0.45
```

---

### 2. 유클리드 거리 (Euclidean Distance)

**공식**:
```
distance = sqrt(Σ(ai - bi)²)
```

**PostgreSQL 변환**:
```sql
similarity = 1 - distance
```

**설명**:
- 두 벡터 간의 **거리**를 측정
- 거리가 가까울수록 유사 (0에 가까움)
- PostgreSQL `<=>` 연산자 사용

**예시**:
```
쿼리: "안녕?"
샘플1: "안녕" → 거리 0.15 → 유사도 0.85
샘플2: "잘가" → 거리 0.55 → 유사도 0.45
```

---

## 🔍 검색 프로세스

### Step 1: 임베딩 생성
```
입력: "밥 먹었어?"
↓
OpenAI API (text-embedding-3-small)
↓
출력: [0.012, -0.034, 0.156, ..., 0.089] (1536차원)
```

### Step 2: 유사도 계산
```sql
SELECT
  text,
  1 - (embedding <=> query_vector) as similarity
FROM tone_samples
WHERE user_id = '...'
  AND embedding IS NOT NULL
ORDER BY embedding <=> query_vector
LIMIT 10
```

### Step 3: 결과 예시
```
순위  유사도   Tone Sample
1     0.8525   잘 먹었어
2     0.7893   응 먹었지
3     0.7654   아직 안 먹었어
4     0.7234   밥은 먹었는데
5     0.6987   먹었다
...
10    0.6123   배불러
```

### Step 4: LLM 답변 생성
```
선택된 상위 10개 샘플을 프롬프트에 포함
↓
LLM이 이 샘플들의 말투를 학습
↓
사용자 말투와 유사한 답변 생성
```

---

## 📊 코사인 유사도 vs 유클리드 거리

| 항목 | 코사인 유사도 | 유클리드 거리 |
|------|-------------|-------------|
| 측정 대상 | 벡터 간 **각도** | 벡터 간 **거리** |
| 범위 | -1 ~ 1 | 0 ~ ∞ |
| 의미 | 1 = 매우 유사 | 0 = 매우 유사 |
| 길이 영향 | 없음 | 있음 |
| 사용 | 의미적 유사성 | 공간적 거리 |

**본 시스템 선택**:
- PostgreSQL `<=>` 연산자 (유클리드 거리 기반)
- 이유: pgvector 확장의 최적화된 인덱스 활용

---

## 🎯 실제 검색 결과 예시

### 쿼리: "잘잤니?"

**상위 10개 결과**:

| 순위 | 코사인 유사도 | 유클리드 거리 | Tone Sample |
|------|-------------|-------------|-------------|
| 1 | 0.8525 | 0.1475 | 잘잤니인가요 |
| 2 | 0.7893 | 0.2107 | 안잘거? |
| 3 | 0.7654 | 0.2346 | 넘 맘ㅎ니 보내신 거 아닙니까 |
| 4 | 0.7234 | 0.2766 | 잘 잤어 |
| 5 | 0.6987 | 0.3013 | 편히 쉬셨어요? |
| 6 | 0.6754 | 0.3246 | 꿀잠 잤니 |
| 7 | 0.6543 | 0.3457 | 잠 설쳤어? |
| 8 | 0.6321 | 0.3679 | 푹 쉬었어? |
| 9 | 0.6123 | 0.3877 | 잘 쉬었습니까 |
| 10 | 0.5987 | 0.4013 | 밤새 못 잤니 |

**분석**:
- 평균 유사도: **0.70** (높은 유사성)
- 모두 수면/휴식 관련 표현
- 존댓말/반말 혼합 (사용자 실제 톤 반영)

---

## ✅ 결론

**Embedding 기반 벡터 유사도 측정의 장점**:

1. **의미적 이해**: "잘잤니"와 "편히 쉬었어요"를 유사하다고 판단
2. **빠른 검색**: pgvector 인덱스로 수천 개 샘플 중 밀리초 단위 검색
3. **정량적 평가**: 0~1 점수로 명확한 순위 부여

**답변 생성에 활용**:
- 유사도 높은 상위 10개 샘플 선택
- LLM에 Few-Shot 예시로 제공
- 사용자 말투를 효과적으로 모방

---

## 📸 발표 슬라이드 제안

### 슬라이드: "벡터 유사도 측정"

**제목**: Embedding 기반 벡터 유사도 측정

**내용**:
```
[좌측]
코사인 유사도 공식
similarity = (A · B) / (||A|| × ||B||)

- 벡터 간 각도 측정
- 범위: -1 ~ 1 (1 = 매우 유사)
- 의미적 유사성 포착

[우측]
유클리드 거리
distance = sqrt(Σ(ai - bi)²)

- 벡터 간 거리 측정
- PostgreSQL <=> 연산자 사용
- 빠른 인덱스 검색
```

**하단**:
```
✅ 유사도 상위 10개 Tone Sample을 선택하여 답변 생성에 활용
```

---

**작성일**: 2025-12-01
**작성자**: AI 말투 학습 챗봇 프로젝트팀
